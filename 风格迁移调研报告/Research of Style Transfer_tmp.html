<!DOCTYPE html>
<html>
<head>
<title>Research of Style Transfer.md</title>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8">

<style>
/* https://github.com/microsoft/vscode/blob/master/extensions/markdown-language-features/media/markdown.css */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

body {
	font-family: var(--vscode-markdown-font-family, -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif);
	font-size: var(--vscode-markdown-font-size, 14px);
	padding: 0 26px;
	line-height: var(--vscode-markdown-line-height, 22px);
	word-wrap: break-word;
}

#code-csp-warning {
	position: fixed;
	top: 0;
	right: 0;
	color: white;
	margin: 16px;
	text-align: center;
	font-size: 12px;
	font-family: sans-serif;
	background-color:#444444;
	cursor: pointer;
	padding: 6px;
	box-shadow: 1px 1px 1px rgba(0,0,0,.25);
}

#code-csp-warning:hover {
	text-decoration: none;
	background-color:#007acc;
	box-shadow: 2px 2px 2px rgba(0,0,0,.25);
}

body.scrollBeyondLastLine {
	margin-bottom: calc(100vh - 22px);
}

body.showEditorSelection .code-line {
	position: relative;
}

body.showEditorSelection .code-active-line:before,
body.showEditorSelection .code-line:hover:before {
	content: "";
	display: block;
	position: absolute;
	top: 0;
	left: -12px;
	height: 100%;
}

body.showEditorSelection li.code-active-line:before,
body.showEditorSelection li.code-line:hover:before {
	left: -30px;
}

.vscode-light.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(0, 0, 0, 0.15);
}

.vscode-light.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(0, 0, 0, 0.40);
}

.vscode-light.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-dark.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 255, 255, 0.4);
}

.vscode-dark.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 255, 255, 0.60);
}

.vscode-dark.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

.vscode-high-contrast.showEditorSelection .code-active-line:before {
	border-left: 3px solid rgba(255, 160, 0, 0.7);
}

.vscode-high-contrast.showEditorSelection .code-line:hover:before {
	border-left: 3px solid rgba(255, 160, 0, 1);
}

.vscode-high-contrast.showEditorSelection .code-line .code-line:hover:before {
	border-left: none;
}

img {
	max-width: 100%;
	max-height: 100%;
}

a {
	text-decoration: none;
}

a:hover {
	text-decoration: underline;
}

a:focus,
input:focus,
select:focus,
textarea:focus {
	outline: 1px solid -webkit-focus-ring-color;
	outline-offset: -1px;
}

hr {
	border: 0;
	height: 2px;
	border-bottom: 2px solid;
}

h1 {
	padding-bottom: 0.3em;
	line-height: 1.2;
	border-bottom-width: 1px;
	border-bottom-style: solid;
}

h1, h2, h3 {
	font-weight: normal;
}

table {
	border-collapse: collapse;
}

table > thead > tr > th {
	text-align: left;
	border-bottom: 1px solid;
}

table > thead > tr > th,
table > thead > tr > td,
table > tbody > tr > th,
table > tbody > tr > td {
	padding: 5px 10px;
}

table > tbody > tr + tr > td {
	border-top: 1px solid;
}

blockquote {
	margin: 0 7px 0 5px;
	padding: 0 16px 0 10px;
	border-left-width: 5px;
	border-left-style: solid;
}

code {
	font-family: Menlo, Monaco, Consolas, "Droid Sans Mono", "Courier New", monospace, "Droid Sans Fallback";
	font-size: 1em;
	line-height: 1.357em;
}

body.wordWrap pre {
	white-space: pre-wrap;
}

pre:not(.hljs),
pre.hljs code > div {
	padding: 16px;
	border-radius: 3px;
	overflow: auto;
}

pre code {
	color: var(--vscode-editor-foreground);
	tab-size: 4;
}

/** Theming */

.vscode-light pre {
	background-color: rgba(220, 220, 220, 0.4);
}

.vscode-dark pre {
	background-color: rgba(10, 10, 10, 0.4);
}

.vscode-high-contrast pre {
	background-color: rgb(0, 0, 0);
}

.vscode-high-contrast h1 {
	border-color: rgb(0, 0, 0);
}

.vscode-light table > thead > tr > th {
	border-color: rgba(0, 0, 0, 0.69);
}

.vscode-dark table > thead > tr > th {
	border-color: rgba(255, 255, 255, 0.69);
}

.vscode-light h1,
.vscode-light hr,
.vscode-light table > tbody > tr + tr > td {
	border-color: rgba(0, 0, 0, 0.18);
}

.vscode-dark h1,
.vscode-dark hr,
.vscode-dark table > tbody > tr + tr > td {
	border-color: rgba(255, 255, 255, 0.18);
}

</style>

<style>
/* Tomorrow Theme */
/* http://jmblog.github.com/color-themes-for-google-code-highlightjs */
/* Original theme - https://github.com/chriskempson/tomorrow-theme */

/* Tomorrow Comment */
.hljs-comment,
.hljs-quote {
	color: #8e908c;
}

/* Tomorrow Red */
.hljs-variable,
.hljs-template-variable,
.hljs-tag,
.hljs-name,
.hljs-selector-id,
.hljs-selector-class,
.hljs-regexp,
.hljs-deletion {
	color: #c82829;
}

/* Tomorrow Orange */
.hljs-number,
.hljs-built_in,
.hljs-builtin-name,
.hljs-literal,
.hljs-type,
.hljs-params,
.hljs-meta,
.hljs-link {
	color: #f5871f;
}

/* Tomorrow Yellow */
.hljs-attribute {
	color: #eab700;
}

/* Tomorrow Green */
.hljs-string,
.hljs-symbol,
.hljs-bullet,
.hljs-addition {
	color: #718c00;
}

/* Tomorrow Blue */
.hljs-title,
.hljs-section {
	color: #4271ae;
}

/* Tomorrow Purple */
.hljs-keyword,
.hljs-selector-tag {
	color: #8959a8;
}

.hljs {
	display: block;
	overflow-x: auto;
	color: #4d4d4c;
	padding: 0.5em;
}

.hljs-emphasis {
	font-style: italic;
}

.hljs-strong {
	font-weight: bold;
}
</style>

<style>
/*
 * Markdown PDF CSS
 */

 body {
	font-family: -apple-system, BlinkMacSystemFont, "Segoe WPC", "Segoe UI", "Ubuntu", "Droid Sans", sans-serif, "Meiryo";
	padding: 0 12px;
}

pre {
	background-color: #f8f8f8;
	border: 1px solid #cccccc;
	border-radius: 3px;
	overflow-x: auto;
	white-space: pre-wrap;
	overflow-wrap: break-word;
}

pre:not(.hljs) {
	padding: 23px;
	line-height: 19px;
}

blockquote {
	background: rgba(127, 127, 127, 0.1);
	border-color: rgba(0, 122, 204, 0.5);
}

.emoji {
	height: 1.4em;
}

code {
	font-size: 14px;
	line-height: 19px;
}

/* for inline code */
:not(pre):not(.hljs) > code {
	color: #C9AE75; /* Change the old color so it seems less like an error */
	font-size: inherit;
}

/* Page Break : use <div class="page"/> to insert page break
-------------------------------------------------------- */
.page {
	page-break-after: always;
}

</style>

<script src="https://unpkg.com/mermaid/dist/mermaid.min.js"></script>
</head>
<body>
  <script>
    mermaid.initialize({
      startOnLoad: true,
      theme: document.body.classList.contains('vscode-dark') || document.body.classList.contains('vscode-high-contrast')
          ? 'dark'
          : 'default'
    });
  </script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({ tex2jax: {inlineMath: [['$', '$']]}, messageStyle: "none" });
</script>
<h1 id="%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%E8%B0%83%E7%A0%94">风格迁移调研</h1>
<!-- TOC -->
<ul>
<li><a href="#%E9%A3%8E%E6%A0%BC%E8%BF%81%E7%A7%BB%E8%B0%83%E7%A0%94">风格迁移调研</a>
<ul>
<li><a href="#%E5%9C%BA%E6%99%AF%E9%9C%80%E6%B1%82">场景需求</a></li>
<li><a href="#%E5%8A%9F%E8%83%BD%E9%9C%80%E6%B1%82">功能需求</a></li>
<li><a href="#%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95">技术方法</a></li>
<li><a href="#%E7%AE%97%E6%B3%95%E8%B0%83%E7%A0%94">算法调研</a>
<ul>
<li><a href="#a-neural-algorithm-of-artistic-style">A Neural Algorithm of Artistic Style</a></li>
<li><a href="#perceptual-losses-for-real-time-style-transfer-and-super-resolution">Perceptual Losses for Real-Time Style Transfer and Super-Resolution</a></li>
<li><a href="#multi-style-generative-network-for-real-time-transfer">Multi-style Generative Network for Real-time Transfer</a></li>
<li><a href="#universal-style-transfer-via-feature-transforms">Universal Style Transfer via Feature Transforms</a></li>
<li><a href="#fast-patch-based-style-transfer-of-arbitrary-style">Fast Patch-based Style Transfer of Arbitrary Style</a></li>
<li><a href="#meta-networks-for-neural-style-transfer">Meta Networks for Neural Style Transfer</a></li>
<li><a href="#arbitrary-style-transfer-in-real-time-with-adaptive-instance-normalization">Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization</a></li>
<li><a href="#coherent-online-video-style-transfer">Coherent Online Video Style Transfer</a></li>
<li><a href="#reconet-real-time-coherent-video-style-transfer-network">ReCoNet: Real-time Coherent Video Style Transfer Network</a></li>
<li><a href="#learning-linear-transformations-for-fast-image-and-video-style-transfer">Learning Linear Transformations for Fast Image and Video Style Transfer</a></li>
<li><a href="#fast-video-multi-style-transfer">Fast Video Multi-Style Transfer</a></li>
<li><a href="#consistent-video-style-transfer-via-compound-regularization">Consistent Video Style Transfer via Compound Regularization</a></li>
</ul>
</li>
<li><a href="#%E5%9F%BA%E4%BA%8Egan">基于GAN</a></li>
<li><a href="#%E8%A1%A5%E5%85%85%E8%B5%84%E6%96%99">补充资料</a>
<ul>
<li><a href="#flownet-learning-optical-flow-with-convolutional-networks">FlowNet: Learning Optical Flow with Convolutional Networks</a></li>
<li><a href="#occusion-mask%E7%9B%B8%E5%85%B3">occusion mask相关</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<!-- /TOC -->
<h2 id="%E5%9C%BA%E6%99%AF%E9%9C%80%E6%B1%82">场景需求</h2>
<ul>
<li>给定视频内容和艺术风格，将二者融合，输出风格转换后的视频</li>
</ul>
<h2 id="%E5%8A%9F%E8%83%BD%E9%9C%80%E6%B1%82">功能需求</h2>
<ul>
<li>模型
<ul>
<li>单模型单风格</li>
<li>单模型多风格/任意风格</li>
</ul>
</li>
<li>实时性
<ul>
<li>实时处理</li>
<li>离线处理</li>
</ul>
</li>
</ul>
<h2 id="%E6%8A%80%E6%9C%AF%E6%96%B9%E6%B3%95">技术方法</h2>
<ul>
<li>传统方法</li>
<li>NST 基于神经网络的风格迁移技术，端到端地生成风格迁移后的图片/视频</li>
</ul>
<h2 id="%E7%AE%97%E6%B3%95%E8%B0%83%E7%A0%94">算法调研</h2>
<h3 id="a-neural-algorithm-of-artistic-style">A Neural Algorithm of Artistic Style</h3>
<ul>
<li>作者：Leon A. Gatys, Alexander S. Ecker, Matthias Bethge</li>
<li>年份：2015</li>
<li>会议：</li>
<li>主要思想：
<ul>
<li>将内容表示和风格表示分开，用预训练好的VGG-16/VGG-19提取内容图片的内容特征C(指定层的特征图输出)和风格图片的风格特征S(指定层的特征图输出经过GRAM矩阵运算后的结果)</li>
<li>将原始随机生成的噪声图片，指定的内容图片和风格图片这三张图片一起输入VGG中，得到原始图片的内容特征C', S'，分别计算CC',SS'之间的欧式距离作为内容损失和风格损失，总损失函数为内容损失和风格损失的加权和，反向迭代原始噪声图片，直到调节噪声图为目标图片。</li>
<li>$\min <em>{I}\left(\lambda</em>{c}\left|\mathbf{C} \mathbf{P}\left(I ; w_{f}\right)-\mathbf{C} \mathbf{P}\left(I_{c} ; w_{f}\right)\right|<em>{2}^{2}+\lambda</em>{s}\left|\mathbf{S} \mathbf{P}\left(I ; w_{f}\right)-\mathbf{S} \mathbf{P}\left(I_{s} ; w_{f}\right)\right|_{2}^{2}\right)$</li>
</ul>
</li>
<li>数据集： 不需要训练数据集，只需要内容图片和风格图片</li>
<li>开源代码：https://github.com/titu1994/Neural-Style-Transfer</li>
<li>评价
<ul>
<li>神经风格迁移(NST)的开山之作，缺点是每次生成图片都要进行迭代，速度太慢，且生成效果不好</li>
</ul>
</li>
</ul>
<h3 id="perceptual-losses-for-real-time-style-transfer-and-super-resolution">Perceptual Losses for Real-Time Style Transfer and Super-Resolution</h3>
<ul>
<li>作者：Justin Johnson, Alexandre Alahi, Li Fei-Fei</li>
<li>年份：2016</li>
<li>会议</li>
<li>主要思想：
<ul>
<li>在Gatys的基础上，在左边加上一个转换网络fw，目标是训练该fw使得对任意输入内容图片，快速地输出该内容图片融合固定地风格图片地结果，单模型单风格快速风格迁移(GTX Titan X GPU, 20FPS for 512X512)。</li>
<li>$\min <em>{w} \sum</em>{I_{c}}\left(\lambda_{c}\left|\mathbf{C P}\left(I_{w} ; w_{f}\right)-\mathbf{C P}\left(I_{c} ; w_{f}\right)\right|<em>{2}^{2}+\lambda</em>{s}\left|\mathbf{S} \mathbf{P}\left(I_{w} ; w_{f}\right)-\mathbf{S} \mathbf{P}\left(I_{s} ; w_{f}\right)\right|_{2}^{2}\right)$</li>
<li>网络结构
<img src="file:///c:/Users/cz/Desktop/desk/style_transfer/风格迁移调研报告/imgs/1.PNG" alt=""></li>
</ul>
</li>
<li>数据集：大量内容图片，如COCO数据集</li>
<li>开源代码：https://github.com/abhiskk/fast-neural-style</li>
<li>评价：
<ul>
<li>速度较快，图像转换质量高，缺点是风格需固定，每增加一个风格都要额外训练一个对应地模型。</li>
</ul>
</li>
</ul>
<h3 id="multi-style-generative-network-for-real-time-transfer">Multi-style Generative Network for Real-time Transfer</h3>
<ul>
<li>作者：Hang Zhang, Kristin Dana.</li>
<li>年份：2017</li>
<li>会议：</li>
<li>主要思想
<ul>
<li>仍然以VGG作为损失评估网络，训练一个生成网络G。G包括一个Siamese Network，用于提取输入风格图片S不同scale的特征统计信息，即不同scale下的Gram矩阵；一个转换网络T，接收内容图片C，C经过T编码后的特征图，通过CoMatch层与风格图片多个scale的特征统计信息进行匹配，得到生成图像。</li>
<li>CoMatch Layer作用在于基于给定的风格图片匹配其特征的二阶统计量。它不是采用传统的C和S的内容损失和风格损失的加权平均，而是引入了一个可学习的矩阵W，能够动态地trade-off 风格与内容的比例系数。并且W的参数从总损失函数学习。
<ul>
<li>CoMatch Layer $$\hat{\mathcal{Y}}^{i}=\Phi^{-1}\left[\Phi\left(\mathcal{F}^{i}\left(x_{c}\right)\right)^{T} W \mathcal{G}\left(\mathcal{F}^{i}\left(x_{s}\right)\right)\right]^{T}$$</li>
<li>Loss Function: $$\begin{aligned} \hat{W}<em>{G} &amp;=\underset{W</em>{G}}{\operatorname{argmin}} E_{x_{c}, x_{s}}{\ &amp; \lambda_{c}\left|\mathcal{F}^{c}\left(G\left(x_{c}, x_{s}\right)\right)-\mathcal{F}^{c}\left(x_{c}\right)\right|<em>{F}^{2} \ &amp;+\lambda</em>{s} \sum_{i=1}^{K}\left|\mathcal{G}\left(\mathcal{F}^{i}\left(G\left(x_{c}, x_{s}\right)\right)\right)-\mathcal{G}\left(\mathcal{F}^{i}\left(x_{s}\right)\right)\right|<em>{F}^{2} \ &amp;\left.+\lambda</em>{T V} \ell_{T V}\left(G\left(x_{c}, x_{s}\right)\right)\right} \end{aligned}$$
<img src="file:///c:/Users/cz/Desktop/desk/style_transfer/风格迁移调研报告/imgs/5.PNG" alt=""></li>
</ul>
</li>
<li>另外，Siamese网络与转换网络的编码器部分共享权重，不需要额外训练。并且增加了新的上采样卷积，避免了反卷积所带来的棋盘效应。通过Resize style image，可以实时更改笔触粗细(Real-time brush-size control)。训练时，可以只训练100个风格，也可以训练到1000个风格，效果基本不变。</li>
</ul>
</li>
<li>开源代码：https://github.com/zhanghang1989/PyTorch-Multi-Style-Transfer</li>
<li>数据集：COCO + 所需要的风格图片集合</li>
<li>评价：MSG-Net实现了单模型多风格转换，并且能够达到实时转换速度，生成图片的质量高。缺点是进行风格迁移时只能使用训练时指定的风格图片集合中的某一个，不能由用户自定义指定。</li>
</ul>
<h3 id="universal-style-transfer-via-feature-transforms">Universal Style Transfer via Feature Transforms</h3>
<ul>
<li>作者：Yijun Li, et al.</li>
<li>年份：2017</li>
<li>会议：</li>
<li>主要思想：
<ul>
<li>作者使用了WCT（Whiten-ColorTransform）将内容图的特征协方差与给定风格的特征协方差进行匹配。与之前的方法相比，该方法不需要针对某一种特征图进行训练，可以使用任意风格图对内容图进行风格转移。</li>
<li>首先将预训练好的VGG作为编码器，针对relu1~relu5层设计和训练5个对应的解码器，其作用是对来自VGG卷积层结果进行图像重构。损失函数为图像重构L2损失和特征L2损失的加权和。 $L=\left|I_{o}-I_{i}\right|<em>{2}^{2}+\lambda\left|\Phi\left(I</em>{o}\right)-\Phi\left(I_{i}\right)\right|_{2}^{2}$</li>
<li>整体流程是从高层(relu5_1)到低层(relu1_1)逐层进行风格对齐和图像重构。WCT层通过白化和着色操作，把内容图片由VGG提取出的特征的协方差矩阵与风格图片对齐，转换后的特征通过之前训练好的重构网络即可生成风格迁移后的图片。
<img src="file:///c:/Users/cz/Desktop/desk/style_transfer/风格迁移调研报告/imgs/3.PNG" alt=""></li>
</ul>
</li>
<li>数据集：COCO</li>
<li>开源代码：https://github.com/Yijunmaverick/UniversalStyleTransfer</li>
<li>评价：这篇文章的思路也是在特征层面进行风格和属性的融合，通过WCT操作进行内容图像到风格图像的协方差矩阵对齐，避开了对一个风格图片训练一个模型的缺点，实现了单模型任意风格的转换。缺点是根据实验结果，其生成图片的质量与其他方法有一定差距，且未提到图像生成速度。</li>
</ul>
<h3 id="fast-patch-based-style-transfer-of-arbitrary-style">Fast Patch-based Style Transfer of Arbitrary Style</h3>
<ul>
<li>作者：Tian Qi Chen, Mark Schmidt</li>
<li>年份：2016</li>
<li>会议:</li>
<li>主要思想
<ul>
<li>抽取内容图片C和风格图片S若干patches,使用预训练好的VGG-19作为特征提取网络，计算patches_i的指定层激活值$\phi_{i}(C)$$\phi_{i}(S)$。对于每个内容图片的patch,根据$$\phi_{i}^{s s}(C, S):=\underset{\phi_{j}(S), j=1, \ldots, n_{s}}{\operatorname{argmax}} \frac{\left\langle\phi_{i}(C), \phi_{j}(S)\right\rangle}{\left|\phi_{i}(C)\right| \cdot\left|\phi_{j}(S)\right|}$$找到一个 closest-matching style patch。将每一个$\phi_{i}(C)$ swap为$\phi_{i}^{s s}(C, S)$,然后将$\phi_{i}^{s s}(C, S)$传入inverse network中生成结果图像，实际上就是根据$\phi_{i}^{s s}(C, S)$进行图像重构。
<img src="file:///c:/Users/cz/Desktop/desk/style_transfer/风格迁移调研报告/imgs/4.PNG" alt=""></li>
<li>训练inverse net时，主要考虑到其重构图片在经过pipeline前半部分的编码输出要尽量接近于$\phi_{i}^{s s}(C, S)$，并且使用全变分正则化保证生成图片具有平滑性。$$\underset{f}{\operatorname{arginf}} \mathbb{E}<em>{H}\left[|\Phi(f(H))-H|</em>{F}^{2}+\lambda \ell_{T V}(f(H))\right]$$</li>
</ul>
</li>
<li>数据集：内容图片数据集COCO 风格图片数据集WikiArt</li>
<li>开源代码：https://github.com/rtqichen/style-swap</li>
<li>评价
<ul>
<li>提出了新的思路即内容图片和风格图片的patches匹配。从作者的实现结果和开源实现来看效果不错，并且能够实现单模型任意风格转换。但是，由于需要在生成网络之前再经过VGG进行特征提取和style-swap，计算完毕后还要将结果送入生成网络重构图片，速度上达不到实时。</li>
</ul>
</li>
</ul>
<h3 id="meta-networks-for-neural-style-transfer">Meta Networks for Neural Style Transfer</h3>
<ul>
<li>作者：Fa long Shen, Shuicheng Yan, Gang Zeng.</li>
<li>年份：2018</li>
<li>会议： CVPR</li>
<li>主要思想
<ul>
<li>
<p>利用大量风格图片训练一个元网络MetaNet,该网络能够根据输入的风格图片对转换网络fw进行参数赋值，输入内容图像到fw中即可快速得到任意内容图片融合任意风格图片的结果(Titan X GPU, 19ms 切换风格，转换网络仅449KB，号称可在移动设备满足实时性)。</p>
</li>
<li>
<p>$\min <em>{\theta} \sum</em>{I_{c}, I_{s}}\left(\lambda_{c}\left|\mathbf{C} \mathbf{P}\left(I_{w_{\theta}} ; w_{f}\right)-\mathbf{C} \mathbf{P}\left(I_{c} ; w_{f}\right)\right|<em>{2}^{2}+\lambda</em>{s}\left|\mathbf{S} \mathbf{P}\left(I_{w_{\theta}} ; w_{f}\right)-\mathbf{S} \mathbf{P}\left(I_{s} ; w_{f}\right)\right|_{2}^{2}\right)$</p>
<ul>
<li>$w_\theta$ 是转换网络的权值，$\theta$是MetaN的权值</li>
<li>$w_{\theta}=\operatorname{MetaN}\left(I_{s} ; \theta\right)$</li>
</ul>
<p><img src="file:///c:/Users/cz/Desktop/desk/style_transfer/风格迁移调研报告/imgs/2.PNG" alt=""></p>
</li>
</ul>
</li>
<li>数据集：内容图片数据集COCO 风格图片数据集WikiArt</li>
<li>开源代码：https://github.com/FalongShen/styletransfer</li>
<li>评价
<ul>
<li>单模型可处理任意风格，速度快，生成质量好</li>
</ul>
</li>
</ul>
<h3 id="arbitrary-style-transfer-in-real-time-with-adaptive-instance-normalization">Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization</h3>
<h3 id="coherent-online-video-style-transfer">Coherent Online Video Style Transfer</h3>
<ul>
<li>
<p>作者：Dongdong Chen, Jing Liao, Lu Y uan2, et al.</p>
</li>
<li>
<p>年份：2017</p>
</li>
<li>
<p>会议：ICCV</p>
</li>
<li>
<p>主要思想</p>
<ul>
<li>通过two-frames 连贯性限制，克服了image style transfer 逐帧转换风格导致视频闪烁的问题。具体做法是，将网络设计为两个子模块，Style Sub-network和Flow Sub-network(分别简称S和F)。S网络是预训练好的风格转换网络，文章中采用Justin Johnson的Transform Net,并将其分为编码器解码器两部分。输入为t-1和t时刻相邻的两帧，前半部分分别得到t-1和t时刻的特征图，对于t-1时刻的特征图$F_{t-1}$，将其与F网络(光流估计网络，文中是在synthetic Flying Chairs数据集上训练好的FlowNet，需要进行fine-tuning以适应当前任务)输出的光流特征$W_t$ warp为$F_t^{'}$,将$F_t^{'}$和$F_t$之差送入MASK模块预测出可追踪的点或区域，这些点或区域仍使用前一帧的特征，否则使用当前帧的特征。输出结果送入S网络的解码器中得到当前帧生成图像。具体pipeline如下所示：
<img src="file:///c:/Users/cz/Desktop/desk/style_transfer/风格迁移调研报告/imgs/7.PNG" alt=""></li>
<li>核心是由短期一致性的propagation实现了整体序列的长期一致性，大大增加了视频的平滑性。
<img src="file:///c:/Users/cz/Desktop/desk/style_transfer/风格迁移调研报告/imgs/6.PNG" alt=""></li>
</ul>
</li>
<li>
<p>数据集:作者自己收集了8个动作电影片段以及youtube上的视频约28,000帧作为训练集，并使用FlowNet2的计算结果作为ground-truth</p>
</li>
<li>
<p>开源代码：暂无</p>
</li>
<li>
<p>评价：该方法有效地解决了逐帧风格迁移的闪烁问题，大大提升了视频风格迁移的图像质量。在之前的视频风格迁移的基础上大幅提升了速度，在GPU Titan X上能达到15FPS。虽然不能在普通设备上实现实时处理，但是对于离线处理是可以接受的选择。缺点是单风格单模型，但是可以通过更换S网络以达到单模型多风格。</p>
</li>
</ul>
<h3 id="reconet-real-time-coherent-video-style-transfer-network">ReCoNet: Real-time Coherent Video Style Transfer Network</h3>
<ul>
<li>作者：Chang Gao, et al.</li>
<li>年份：2018</li>
<li>会议：ACCV</li>
<li>主要思想：
<ul>
<li>核心在于损失函数的设计。作者同时考虑了Justin Jhonson论文中的perceptual loss和相邻帧之间的temporal loss, 既保证生成图像的质量，又能保证风格转换后的视频帧的平滑性。其temporal loss又考虑到两方面，一是提出luminance warping constraint，考虑到了可追踪像素的亮度变化(光流估计中亮度不变假设在实际视频中并不是一定成立)，提高风格转换的稳定性并抑制temporal inconsistency；二是提出特征图层面的 temporal loss，抑制相同物体在编码器中的高层特征的误差，提高了可追踪点风格转换的稳定性。</li>
<li>与Coherent Online Video Style Transfer中在inference阶段使用光流不同，ReCoNet不需要在inference阶段使用光流，仅仅是在训练阶段使用了光流的ground-truth，因而大大加快的了推理速度，该模型号称能在GTX 1080Ti 单GPU下达到200FPS.
<img src="file:///c:/Users/cz/Desktop/desk/style_transfer/风格迁移调研报告/imgs/11.PNG" alt=""></li>
<li>损失函数
<ul>
<li>
<p>$\mathcal{L}<em>{t e m p, o}(t-1, t)=\sum</em>{c} \frac{1}{D} M_{t}\left|\left(O_{t}-W_{t}\left(O_{t-1}\right)\right)<em>{c}-\left(I</em>{t}-W_{t}\left(I_{t-1}\right)\right)_{Y}\right|^{2}$</p>
<ul>
<li>c ∈ [R, G, B] is each of the RGB channels of the image, Y the relative
luminance channel,D =
H ×W</li>
</ul>
</li>
<li>
<p>$\mathcal{L}<em>{t e m p, f}(t-1, t)=\frac{1}{D} M</em>{t}\left|F_{t}-W_{t}\left(F_{t-1}\right)\right|^{2}$</p>
</li>
<li>
<p>$\begin{array}{r}
\mathcal{L}(t-1, t)=\sum_{i \in{t-1, t}}\left(\alpha \mathcal{L}<em>{\text {content}}(i)+\beta \mathcal{L}</em>{\text {style}}(i)+\gamma \mathcal{L}<em>{t v}(i)\right)
+\lambda</em>{f} \mathcal{L}<em>{\text {temp}, f}(t-1, t)+\lambda</em>{o} \mathcal{L}_{\text {temp}, o}(t-1, t)
\end{array}$</p>
</li>
</ul>
</li>
</ul>
</li>
<li>数据集：Monkaa and FlyingThings3D in the Scene Flow datasets(training), MPI Sintel(test)</li>
<li>开源代码:https://github.com/safwankdb/ReCoNet-PyTorch</li>
<li>评价：该方案综合了视频风格转换的帧间平滑性和推理的实时性，且效果较好，有完整的开源实现以及测试样例。缺点是单风格单模型。</li>
</ul>
<h3 id="learning-linear-transformations-for-fast-image-and-video-style-transfer">Learning Linear Transformations for Fast Image and Video Style Transfer</h3>
<ul>
<li>待完成</li>
</ul>
<h3 id="fast-video-multi-style-transfer">Fast Video Multi-Style Transfer</h3>
<ul>
<li>作者: Wei Gao1, Yijun Li, Yihang Yin,Ming-Hsuan Yang</li>
<li>年份：2020</li>
<li>会议：WACV</li>
<li>主要思想
<ul>
<li>在一个网络模型中去学习多种不同的风格。在卷积层后添加multi-instance normalization layer，使用MIN-Layer将特征转换到新的风格特征空间，可由指定index控制。
<img src="file:///c:/Users/cz/Desktop/desk/style_transfer/风格迁移调研报告/imgs/9.PNG" alt=""></li>
<li>插入两个ConvLSTM模块到Encoder-Decoder中，使得风格化结果更具有时间连贯性。</li>
<li>设计另外一个光流估计的分支FlowNetS，类似于上述的Coherent Online Video Style Transfer中的Flow Sub-network，使得t-1时刻和t时刻的帧更具有长短期连贯性。
<img src="file:///c:/Users/cz/Desktop/desk/style_transfer/风格迁移调研报告/imgs/8.PNG" alt=""></li>
<li>实时性
<img src="file:///c:/Users/cz/Desktop/desk/style_transfer/风格迁移调研报告/imgs/10.PNG" alt=""></li>
</ul>
</li>
<li>数据集：COCO + 指定的风格图片集合</li>
<li>开源代码：https://github.com/gaow0007/Fast-Multi-Video-Style-Transfer</li>
<li>评价：该文章满足单模型多风格的视频风格转换，且速度快。</li>
</ul>
<h3 id="consistent-video-style-transfer-via-compound-regularization">Consistent Video Style Transfer via Compound Regularization</h3>
<ul>
<li>待完成</li>
</ul>
<h2 id="%E5%9F%BA%E4%BA%8Egan">基于GAN</h2>
<ul>
<li>待完成</li>
</ul>
<h2 id="%E8%A1%A5%E5%85%85%E8%B5%84%E6%96%99">补充资料</h2>
<h3 id="flownet-learning-optical-flow-with-convolutional-networks">FlowNet: Learning Optical Flow with Convolutional Networks</h3>
<ul>
<li>
<p>作用：用CNN预测光流的经典论文</p>
</li>
<li>
<p>作者：Philipp Fischer, et al.</p>
</li>
<li>
<p>年份: 2015</p>
</li>
<li>
<p>会议: ICCV</p>
</li>
<li>
<p>主要思想</p>
<ul>
<li>首次将CNN运用到光流估计上。输入为t和t+1两个时刻的帧，通过FlowNet，计算每个像素的光流，即t+1时刻相对于t时刻该像素点的为一向量(x,y)。所以总体计算出的光流是和原图像大小相等的双通道图像。</li>
<li>提出了两种网络结构FlowNetSimple和FlowNetCorr，这里主要分析第二种。FlowNetCorr对t和t+1两个时刻的帧送入两个分支分开处理，得到各自的特征图，然后使用correlation layer找到这两个特征图之间的联系并合并成一条路。计算两个特征图之间的联系使用如下公式。实际上相当于对特征图f1和特征图f2的每个patch两两做卷积，但这样复杂度比较高，于是限制每个patch只与它周围D个位置patch做联系运算。
$$c\left(\mathbf{x}<em>{1}, \mathbf{x}</em>{2}\right)=\sum_{\mathbf{o} \in[-k, k] \times[-k, k]}\left\langle\mathbf{f}<em>{1}\left(\mathbf{x}</em>{1}+\mathbf{o}\right), \mathbf{f}<em>{2}\left(\mathbf{x}</em>{2}+\mathbf{o}\right)\right\rangle $$</li>
</ul>
<p><img src="file:///c:/Users/cz/Desktop/desk/style_transfer/风格迁移调研报告/imgs/12.PNG" alt=""></p>
<ul>
<li>为了得到大量的数据集，作者自己设计了Flying Chairs。由于人工标注每个像素点光流的GT基本不可能，故作者使用了一种巧妙的方法(类似于先斩后奏)：首先生成一些椅子图片，融合到背景当中去，为了产生运动信息，产生融合图片的时候会随机产生一个位移变量，与背景和椅子的位移相关，再通过这个位移变量产生第二个图片和光流。
<img src="file:///c:/Users/cz/Desktop/desk/style_transfer/风格迁移调研报告/imgs/13.PNG" alt=""></li>
</ul>
</li>
</ul>
<h3 id="occusion-mask%E7%9B%B8%E5%85%B3">occusion mask相关</h3>
<ul>
<li>待完成</li>
</ul>

</body>
</html>
